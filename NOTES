
IMPLEMENTATION NOTES

arrays and channels
    - these needs pools and handles like what i've done for scalars
    - likewise for hashes
    - does it make sense for a scalar to contain a channel reference?
    - when writing to a channel that is already full, should we:
        - block until there is space available to write to
            - this makes some sense: if the consumer is not consuming, the producer should not produce
            - but if the producer is, for example, keyboard input, we want to let them keep typing until the other side catches up
            - if the consumer is in a state where it will never read from the channel, this will lead to deadlock with the
              producer doing nothing (but also not consuming any cpu)
        - block for a while, but if no space becomes available then grow the channel
            - how long to wait?
                - configurable "aggression" set by channel flags?
                - exponential backoff?
            - if the consumer is in a state where it will never read from the channel, this will lead to the producer slowly
              growing the channel towards memory exhaustion, though without as much load on the cpu
        - immediately grow the channel
            - could rapidly lead to memory exhaustion if the consumer is waiting on something else, which might lead the the
              consumer itself failing and the whole thing falling over
            - if the consumer is in a state where it will never read from the channel, this will lead to the producer spinning
              hard until memory is exhausted and it crashes

local variables stored on data stack -- no
    - this is re-entrant, since each invocation of the function has its own data stack
    - if scope is based on functions (not arbitrary blocks) then don't need to worry about needing to access a parent scope's 
      locals
    - but if nested functions are implemented, then we -do- need a way to refer to a variable from a containing scope
        - one possibility would be quietly promoting any locals that are referenced from a nested function to globals
            - but this isn't re-entrant: all invocations of the function now unexpectedly refer to the same shared instance for 
              globals promoted in this way.
        - and then data stack accesses need to be atomic, cause multiple threads might be hitting it


all variables stored in pools
    - need to reference count them -- how?
        - does simply being in someone's scope count as a reference, or only if you explicitly take a reference?
          e.g. does  "my $foo" require a non-zero reference count at wherever $foo is allocated, or does the refcount only 
          become non-zero when someone later says "my $bar = \$foo" ?
            - any reference, including the initial one, is counted
    - can intertwine a free list so the pool space can be reclaimed as locals go out of scope
        - just have a "type" value of "FREE" and another field in the union which is a pointer to the next free pool entry.
          the wrapper struct for the pool would have a head pointer
    - each invocation of a function is going to have its locals stored at a different location in the pool, so we need
      a symbol table so they can refer to them consistently.  each local would get a globally unique identifier.  the bytecode
      for each function would initially arrange storage for its locals and register them in its symbol table.  operations for
      reading/writing to locals would then take an identifier as a parameter and look up the symbol table to find the right
      place to read/write to
        - need some tidy data structure for the symbol table.  keys would be the unique identifiers (i.e. just ints), values
          would be an index into the pool (so that the pool can be grown/reallocated elsewhere without invalidating pointers)
        - but this symbol table doesn't help with that, cause each invocation is going to refer to the same unique identifier,
          which is going to refer to the same in the pool, so it's completely non re-entrant
            - unless each function invocation has its own symbol table, just like it has its own data stack
                - but this is then not really much different from just keeping locals on the data stack
                - though i guess the symbol table is easier to scan upwards if an identifier doesn't exist at the current scope
                    - i think it would need the same sort of spaghetti stack setup that the data stack uses so that nested 
                      functions/closures/coroutines can access their ancestors' locals
                        - does this mean the data stack no longer needs the spaghetti stack setup?  it still needs a parent
                          pointer to get back to the old scope when the function returns
    - globals can be stored in the same pool.  they just don't get freed until program end.
    - each function's data stack then is only used for transient data between operations
        - which i think means its only ever accessed by the thread the function is running in
            - which i think means data stack operations don't need to be atomic, and i can get rid of the locking infrastructure
    - do pool accesses need to be atomic?
        - read-write locking on the entire pool? some smaller granularity (a la global_t?)
            - need to prevent multiple threads simultaneously grabbing an item from the free list - solved by locking the free
              list.  don't need to lock the whole pool, cause once scalars are allocated they're either shared and individually
              locked (see below) or they're not shared and won't be accessed concurrently
        - add a "shared" flag and a mutex pointer to pooled_scalar_t.  accesses to scalars that have this flag set should
          lock/unlock the mutex, while accesses to other scalars don't incur the overhead. 
            - all globals, and any local variables refered to by nested functions, should be created with the shared flag.  
            - any other conditions where this is needed?
            - then get_scalar or whatever it's called function needs to take a set of flags to apply to the newly allocated scalar
    - what happens when the pool is full?  
        - reallocate its contents to a larger size? would need to lock the entire pool so nothing tries to access it during the 
          resize
            - actually, i think i only need to lock the free list, cause that's the only thing that changes from the client 
              perspective.  doesn't matter if they access the new copy of the array or the old copy of the array
        - allow multiple pools to exist at once? how do you uniquely identify a single variable in a single pool?
    - if all scalar ops are contextual and variables aren't stored on the data stack, does the data stack need to be scalars, or
      can it just be intptr_t's a la forth cells?
    - might need to bring old scalar_* functions back and move current scalar_* stuff to pooled_scalar_* or similar
        - maybe call the basic unpooled version anon_scalar_t?
        - array_t, channel_t and data_stack_t all want access to scalars that aren't memory managed
            - or should array_t's members be memory managed scalars? (i.e. an array of scalar_handle_t)
                - then you'd be able to reference individual elements, which might be useful in some loop/call constructions

symbol table
    - needs to map int identifiers to int values (which in turn are pool indexes)
    - needs to be growable?
        - does it need to be shrinkable?
    - quick lookup of identifiers
    - can it really just be an array?
        - no, because identifiers need to be globally unique
    - newer identifiers would generally be higher than older ones, which would lead to less than optimal binary tree 
      performance... unless the compiler allocates them randomly
    - should this just be part of the data stack -- turn the whole shebang into a "context" sorta thing and have both the
      transient data stack and the symbol table as part of the same unit?  since everywhere one is used the other is too, and they
      both seem to need the spaghetti/registry stuff too, so it would save writing a lot of the same code twice.
        - how does return stack fit into that?
        - does the data stack actually need all that scoping shenanigans now that there's a symbol table for locals?
    - what if the bytecode provides an instruction to clone a symbol from an ancestor scope, so that two scopes can have the same
      identifier with the same name pointing to the same object?
        - it would need to increase the refcount on the object pointed to
        - the object pointed to would need to be shared
        - the only scopes that would be able to do this would be ones that could see the identifier anyway, but it would mean
          they would only need to climb the scope heirarchy once to clone it, instead of at every access
        - this might permit parent scopes to be cleaned up while their child scopes are still active, but only if the child made
          sure to clone every symbol it needed before the parent ended
          - what if there was also an instruction for a child to emancipate itself from its parent?
            - decrement parent's refcount, and set child's parent pointer to NULL
            - would still need the usual registry/garbage collection mechanism in case other children don't do this
            - it would mean that the child is guaranteed that its parent scope will remain until it doesn't need it anymore
            - in the typical case, when the parent ends it can clean itself up right away without waiting for garbage collection
            - even if the parent's refcount reaches zero when the child isolates itself, the child should not automatically 
              clean up the parent -- there might still be pointers to it elsewhere.  leave it to the garbage collector.
    - do symbol tables need mutexes?
        - i think so, at table level.  writes only happen at the "current" scope, but if a child is running in a separate
          context, its parent might be messing around with its own scope at the same time as the child wants to search upwards
          through it.
    - need a way to way to say "here, i have this object, associate an identifier with it". neither symbol_define nor 
      symbol_clone does quite what i want.  for use when a function wants to grab its parameters from the stack and squirrel
      them away such that it can refer to them later by identifier without a lot of stack gymnastics to keep track of where they
      are.
            
        

bytecode operations
    - SRLOCK/SRUNLOCK (and ditto for AR/HR/CR...) to expose lock/unlock semantics to the user
        - if this is exposed, it's not safe for scalar_*, channel_*, array_*, hash_* etc functions to do their own
          lock/unlocking, cause the thread might already hold that mutex.
            - if they don't do their own locking, there's not much they do need to do, so particularly for scalars, it would be
              nice if they could just be inlined wrappers around the actual anon_scalar_* functions doing the work
                - this would need changes to the pool so that the pool singletons are extern rather than static.  pool.h would
                  declare its singleton as extern whatever in the DEFINITIONS macro, and would also need to provide a macro that
                  the actual module responsible for the singleton can use to define the actual object in its translation unit.
        - what's the use case in terms of ( -- ) for these ops, and how would someone use them to perform an actual task (e.g.
          lock the referenced object, read its current value, do some work, set a new value, unlock)
    - SYMDEF should return the reference created (or undef on failure)
        - needs symbol_define to return the symbol object so it can interrogate its properties
    - INTGT0, INTLT0, INTEQ0 - and similar for floats
        - maybe don't do INTEQ0 - it's just an int-specific version of NOT, so it's kind of a waste of an instruction
    - AND OR XOR NOT - boolean, not bitwise
    - INTINCR, INTDECR
    - IN (like OUT)
        - what about INL (like OUTL)?
        - should it try and do clever conversion to int/float at input time, or just leave it as a string till you try to use
          it as something else?
            - i guess, putting it that way, leave it as a string, and convert on demand
        - replace all this with proper io_t handles once they're implemented: 
            - IN and OUT should each require an io reference from the stack to read/write from
            - let the language on top handle defaulting to stdin etc
    - CORO, FRCORO for spawning new threads
        - need to track threads and contexts for garbage collection?
        - should vm_execute free the context and exit the thread before it returns?
            - it can't free the thread because it is the thread
        - should the context_t object store the thread id?
    - compare two scalars for equality -- numeric and stringwise, like == and eq
    - scalars
        - chomp a delimiter off the end of a string?
        - split a string into an array of characters
        - join an array of characters into a string
        - byte with a single byte number, and rename intlit, strlit, fltlit to int str flt
        - replace an int with its char value and vice-versa
    - hashes and arrays:
        - put contents of an array onto the stack
        - put keys from a hash onto the stack
        - put values from a hash onto the stack
        - put keys and values from a hash onto the stack
        - populate an array from the stack
        - populate a hash from the stack
        - reverse a list of items on the stack
    - stream i/o
        - IN:  takes a stream ref and a number of bytes to read from the stream, puts back the value read
        - INL: takes a stream ref and an integer delimiter value, puts back the value read including the delimiter
        - OUT: takes a stream ref and a value to output, outputs just the value
        - OUTL: takes a stream ref, a value to output, and a delimiter; outputs the value followed by the delimiter


threading
    - vm_execute takes a context_t* as an argument with the context to execute
        - if it returns its own context_t*, then whoever instantiated it doesn't need to worry about cleaning it up, it
          can be cleaned up when the thread is joined
            - useless if it's detached
    - threads need to be joined to reclaim their resources, or they need to be detached so they self-clean upon exit
        - make main (or equivalent) join all threads before it exits?
            - resources don't get released until exit - not great for long-running programs
            - need somewhere to track all the pthread_t's so that main can find and join them all
        - detatch all threads?
            - then each thread's resources will be released as soon as it finishes
            - who releases the context_t?
                - the thread itself before it exits?
                    - looks like!
            - if the threads are detached, their pthread_t's can't be referrred to by any other thread afaict
                - does this mean they don't need to be tracked anywhere?
                - how to manage storage for them?  
                    - if it's heap allocated, the thread can't release it before it finishes
                    - but if it's automatic/stack allocated, what happens when it goes out of the caller's scope?
                        - pthreads don't use the pthread_t themselves, it's just a handle for me to use.  so it can safely
                          live in automatic storage and go out of scope with the caller.
        - vm_main tries to clean up after itself when the main execution context ends, but this causes problems for any
          other threads that still exist and are probably trying to access data.
            - make vm_main wait for the other threads to finish?  
                - vm.c would need a registry of contexts, and context_destroy would remove it from the registry and possibly
                  signal a condition variable
                    - vm_main would then be able to spin on the registry or wait on the condition, and only do final cleanup
                      once all the other contexts are finished
                    - this is a bit messy cause i'd need to set up a condition variable and signalling just so a thread can say
                      that it's ended, and the data in the context isn't actually useful to vm_main anyway
                - or it could have a registry of pthread_t's, and any that are still around at the end of vm_main get joined
                    - once vm_execute finishes running bytecode, it would then remove its own pthread_t from the registry
                      and call pthread_detach on it.  then when it ends a moment later it'll be automatically cleaned up
                    - then, when vm_main finishes, it loops over the remaining pthread_t's in the registry and joins on them all
                        - but, when they all finish they're each going to detach themselves -- what happens if a thread detaches
                          itself while another thread is waiting to join it?
                            - can't find any clear statement about what happens in this case, so it's probably undefined :/
                - if a thread is in an infinite loop, vm_main will never end.  caller needs to make sure their threads have
                  a finish (e.g. by tryreading on a control channel)
            - make vm_main kill all the other threads
                - would need to know pthread_t's of the threads that are currently active -- could store them in a registry
                    - vm_execute could call pthread_detach(pthread_self()) right before it finishes, so that any threads that
                      do manage to finish on their own disappear quietly, and any that remain when vm_main finishes can be
                      cancelled
                - but vm_main doesn't know whether it's safe to just terminate the threads - what if they're holding some
                  resource that needs to be properly released (e.g. db handle)?
    - should my mutexes be recursive?
        - it would allow individual accesses to shared objects to be locked atomically, while also allowing operations
          like SRLOCK/SRUNLOCK to exist for larger user-defined atomic operations -- seems like a good idea.
            - the foo_pool_t struct would need a pthread_mutex_attr_t that it uses for allocating mutexes for shared objects
                - this would need to be initialised by the foo_pool_INIT function and cleaned up by foo_pool_DESTROY
                - you'd need to pass the desired mutex type as an argument to POOL_HEADER_CONTENTS
        - CAN NOT be used with condition variables, so channel_t must use an ordinary mutex due to its use of condition
          variables.  but pooled scalars can safely use a recursive mutex, because they do not make any use of condition
          variables.

assembler
    - parse hexadecimal and octal numbers (leading zero)
    - parse single characters in '' as integers?

