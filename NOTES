

memory management and things

local variables stored on data stack
    - this is re-entrant, since each invocation of the function has its own data stack
    - if scope is based on functions (not arbitrary blocks) then don't need to worry about needing to access a parent scope's 
      locals
    - but if nested functions are implemented, then we -do- need a way to refer to a variable from a containing scope
        - one possibility would be quietly promoting any locals that are referenced from a nested function to globals
            - but this isn't re-entrant: all invocations of the function now unexpectedly refer to the same shared instance for 
              globals promoted in this way.
        - and then data stack accesses need to be atomic, cause multiple threads might be hitting it


all variables stored in pools
    - need to reference count them -- how?
        - does simply being in someone's scope count as a reference, or only if you explicitly take a reference?
          e.g. does  "my $foo" require a non-zero reference count at wherever $foo is allocated, or does the refcount only 
          become non-zero when someone later says "my $bar = \$foo" ?
    - can intertwine a free list so the pool space can be reclaimed as locals go out of scope
        - just have a "type" value of "FREE" and another field in the union which is a pointer to the next free pool entry.
          the wrapper struct for the pool would have a head pointer
    - each invocation of a function is going to have its locals stored at a different location in the pool, so we need
      a symbol table so they can refer to them consistently.  each local would get a globally unique identifier.  the bytecode
      for each function would initially arrange storage for its locals and register them in its symbol table.  operations for
      reading/writing to locals would then take an identifier as a parameter and look up the symbol table to find the right
      place to read/write to
        - need some tidy data structure for the symbol table.  keys would be the unique identifiers (i.e. just ints), values
          would be an index into the pool (so that the pool can be grown/reallocated elsewhere without invalidating pointers)
        - but this symbol table doesn't help with that, cause each invocation is going to refer to the same unique identifier,
          which is going to refer to the same in the pool, so it's completely non re-entrant
            - unless each function invocation has its own symbol table, just like it has its own data stack
                - but this is then not really much different from just keeping locals on the data stack
                - though i guess the symbol table is easier to scan upwards if an identifier doesn't exist at the current scope
                    - i think it would need the same sort of spaghetti stack setup that the data stack uses so that nested 
                      functions/closures/coroutines can access their ancestors' locals
                        - does this mean the data stack no longer needs the spaghetti stack setup?  it still needs a parent
                          pointer to get back to the old scope when the function returns
    - globals can be stored in the same pool.  they just don't get freed until program end.
    - each function's data stack then is only used for transient data between operations
        - which i think means its only ever accessed by the thread the function is running in
            - which i think means data stack operations don't need to be atomic, and i can get rid of the locking infrastructure
    - do pool accesses need to be atomic?
        - read-write locking on the entire pool? some smaller granularity (a la global_t?)
            - need to prevent multiple threads simultaneously grabbing an item from the free list - solved by locking the free
              list.  don't need to lock the whole pool, cause once scalars are allocated they're either shared and individually
              locked (see below) or they're not shared and won't be accessed concurrently
        - add a "shared" flag and a mutex pointer to pooled_scalar_t.  accesses to scalars that have this flag set should
          lock/unlock the mutex, while accesses to other scalars don't incur the overhead. 
            - all globals, and any local variables refered to by nested functions, should be created with the shared flag.  
            - any other conditions where this is needed?
            - then get_scalar or whatever it's called function needs to take a set of flags to apply to the newly allocated scalar
    - what happens when the pool is full?  
        - reallocate its contents to a larger size? would need to lock the entire pool so nothing tries to access it during the 
          resize
            - actually, i think i only need to lock the free list, cause that's the only thing that changes from the client 
              perspective.  doesn't matter if they access the new copy of the array or the old copy of the array
        - allow multiple pools to exist at once? how do you uniquely identify a single variable in a single pool?
    - if all scalar ops are contextual and variables aren't stored on the data stack, does the data stack need to be scalars, or
      can it just be intptr_t's a la forth cells?
    - might need to bring old scalar_* functions back and move current scalar_* stuff to pooled_scalar_* or similar
        - maybe call the basic unpooled version anon_scalar_t?
        - array_t, channel_t and data_stack_t all want access to scalars that aren't memory managed
            - or should array_t's members be memory managed scalars? (i.e. an array of scalar_handle_t)
                - then you'd be able to reference individual elements, which might be useful in some loop/call constructions
